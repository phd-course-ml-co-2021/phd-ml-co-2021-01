{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Alternating Direction Method of Multipliers\n",
    "\n",
    "From the past, with vengeance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Operator Splitting Quadratic Programming\n",
    "\n",
    "**We will tackle our QP using the [OSQP](https://osqp.org/) solver by Oxford University**\n",
    "\n",
    "OSQP is a modern solver for Quadratic Programs in the form:\n",
    "$$\n",
    "\\arg\\min_x \\left\\{\\frac{1}{2} x^T P x + q^T x \\mid l \\leq Ax \\leq u \\right\\}\n",
    "$$\n",
    "\n",
    "The solver:\n",
    "\n",
    "* is [very fast](https://pypi.org/project/qpsolvers/), especially for problems with sparse matrices\n",
    "* is available under a (very permissive) Apache 2.0 license\n",
    "* has API for many programming languages\n",
    "\n",
    "**The solver relies on the Alternating Direction Method of Multipliers (ADMM)**\n",
    "\n",
    "* ...Plus [a bunch of clever \"tricks\"](https://osqp.org/citing/) to improve speed\n",
    "* Here we will discuss only the basic ADMM, to provide _an intuition_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Alternating Direction Method of Multipliers\n",
    "\n",
    "**The [ADMM](https://dl.acm.org/doi/abs/10.1561/2200000016) solves numerical constrained optimization problems in the form:**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin} \\ & f(x) + g(z) \\\\\n",
    "\\text{subject to: } & Ax + Bz = c\n",
    "\\end{align}$$\n",
    "\n",
    "* Where $f$ and $g$ are assumed to be convex\n",
    "\n",
    "**The methods relies on a so-called _augmented Lagrangian_**\n",
    "\n",
    "This is a reformulation where the constraints are turned into _penalty terms_:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\rho}(x, z, \\lambda) = f(x) + g(z) + \\lambda^T(Ax+Bz-c) + \\frac{1}{2}\\rho \\|Ax+Bz-c\\|_2^2\n",
    "$$\n",
    "\n",
    "* The algorithm idea is to _optimize the augmented Lagrangian_\n",
    "* ...And to encourage constraint satisfaction via the penalty terms\n",
    "* In practice, this is done by adjusting the _multiplier vector $\\lambda$_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Alternating Direction Method of Multipliers\n",
    "\n",
    "**The ADMM operates as follows**\n",
    "\n",
    "We start from an initial assignment $x^{0}, z^{0}, \\lambda^{0}$, then we iterate:\n",
    "\n",
    "$$\\begin{align}\n",
    "x^{k+1} & = \\text{argmin}_x \\mathcal{L}_{\\rho}(x, z^k, \\lambda^k) \\\\\n",
    "z^{k+1} & = \\text{argmin}_z \\mathcal{L}_{\\rho}(x^{k+1}, z, \\lambda^k) \\\\\n",
    "\\lambda^{k+1} & = \\lambda^k + \\rho (Ax^{k+1}+Bz^{k+1}-c)\n",
    "\\end{align}$$\n",
    "\n",
    "In other words:\n",
    "\n",
    "* We keep everything fixed and we optimize over $x$ to obtain $x^{k+1}$\n",
    "* We replace $x^{k}$ with $x^{k+1}$, keep everything fixed and optimize over $z$\n",
    "* Finally, we update the multiplier vector\n",
    "\n",
    "**The switch between $x$ and $z$ optimization is the \"alternating\" part**\n",
    "\n",
    "...While the use of the multipliers $\\lambda$ explains the rest of the name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiplier Update\n",
    "\n",
    "**Let's try to understand better the multiplier update**\n",
    "\n",
    "...Which consists in the rule:\n",
    "$$\n",
    "\\lambda^{k+1} = \\lambda^k + \\rho (Ax^{k+1}+Bz^{k+1}-c)\n",
    "$$\n",
    "\n",
    "* The term $Ax^{k+1}+Bz^{k+1}-c$ is just the current constraint violation\n",
    "* ...In particular both its _amount_ and _direction_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**If $(Ax^{k+1}+Bz^{k+1})_i > c_i$ for some constraint $i$:**\n",
    "\n",
    "* Then we _increase_ the corresponding multiplier $\\lambda_i$\n",
    "* So that the penalty term $\\lambda_i (Ax^{k+1}+Bz^{k+1}-c)_i$ grows\n",
    "* This will push the next iteration to reduce the degree of violation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiplier Update\n",
    "\n",
    "**Let's try to understand better the multiplier update**\n",
    "\n",
    "...Which consists in the rule:\n",
    "$$\n",
    "\\lambda^{k+1} = \\lambda^k + \\rho (Ax^{k+1}+Bz^{k+1}-c)\n",
    "$$\n",
    "\n",
    "* The term $Ax^{k+1}+Bz^{k+1}-c$ is just the current constraint violation\n",
    "* ...In particular both its _amount_ and _direction_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**If $(Ax^{k+1}+Bz^{k+1})_i < c_i$ for some constraint $i$:**\n",
    "\n",
    "* Then we _decrease_ the corresponding multiplier $\\lambda_i$\n",
    "* So that the penalty term $\\lambda_i (Ax^{k+1}+Bz^{k+1}-c)_i$ grows (again)\n",
    "* This will push the next iteration to reduce the degree of violation (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiplier Update\n",
    "\n",
    "**Let's try to understand better the multiplier update**\n",
    "\n",
    "...Which consists in the rule:\n",
    "$$\n",
    "\\lambda^{k+1} = \\lambda^k + \\rho (Ax^{k+1}+Bz^{k+1}-c)\n",
    "$$\n",
    "\n",
    "* The term $Ax^{k+1}+Bz^{k+1}-c$ is just the current constraint violation\n",
    "* ...In particular both its _amount_ and _direction_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**If $(Ax^{k+1}+Bx^{k+1})_i = c_i$ for some constraint $i$:**\n",
    "\n",
    "* Then we _keep_ the corresponding multiplier $\\lambda_i$ _as it is_\n",
    "* The constraint is not violated, so there is nothing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main Advantages of the Method\n",
    "\n",
    "The method has _two major advantages_:\n",
    "\n",
    "**1) The $x$ and $z$ variables can be handled _in isolation_**\n",
    "\n",
    "* This results into simpler problems\n",
    "* ...And in some cases enables massive parallelization\n",
    "\n",
    "**2) The ADMM converges under relatively _mild conditions_**\n",
    "\n",
    "* In the classical formulation, $f$ and $g$ need to be _closed_, _proper_, _convex_ functions\n",
    "  - They do _not_ need to be differentiable\n",
    "  - They _can_ take the value $+\\infty$\n",
    "  - We will see why that matters in the next slides\n",
    "* The second condition is that $\\mathcal{L}_{0}(x, z, \\lambda)$ should have a saddle point\n",
    "  - This one is way trickier to check...\n",
    "\n",
    "The full convergence proof can be found e.g. [here](https://dl.acm.org/doi/abs/10.1561/2200000016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The ADMM and QP\n",
    "\n",
    "That was the whole point, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QP Reformulation\n",
    "\n",
    "**Let's see these advantages at work on Quadratic Programs**\n",
    "\n",
    "We need to solve:\n",
    "$$\n",
    "\\text{argmin}_x \\left\\{\\frac{1}{2}x^T P x + q^T x \\mid l \\leq Ax \\leq u \\right\\}\n",
    "$$\n",
    "\n",
    "...Which we reformulate to:\n",
    "$$\\begin{align}\n",
    "\\text{argmin } & x^TPx + q^Tx \\\\\n",
    "\\text{subject to: } & z = Ax \\\\\n",
    "& l \\leq z \\leq u\n",
    "\\end{align}$$\n",
    "\n",
    "* We have introduced _a new variables $z$_\n",
    "* ...And posted the inequality constraints over that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QP Reformulation\n",
    "\n",
    "**Then, we turn the inequality constraints into a function**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin } & x^TPx + q^Tx + \\chi_{l \\leq z \\leq u}(z) \\\\\n",
    "\\text{subject to: } & z = Ax\n",
    "\\end{align}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\chi_{l \\leq z \\leq u}$ is the _characteristic function_ of $l \\leq z \\leq u$\n",
    "  - It's value is $+\\infty$ when the constraint is violated and 0 elsewhere\n",
    "  - In this case, it is non-differentiable, but closed, proper, and convex!\n",
    "* $x^TPx + q^Tx$ is our usual cost term\n",
    "  - It is differentiable\n",
    "  - ...And closed, proper, and convex if $P$ is semi-definite positive\n",
    "\n",
    "**We can now proceed to apply the ADMM!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The ADMM Steps for QP\n",
    "\n",
    "**We need to start from a feasible $x^0, z^0, \\lambda^0$:**\n",
    "\n",
    "* That's easy, we get it by setting $\\lambda^0 = 0$, $z^0 = l$, then solving $A x^0 = l$\n",
    "\n",
    "**The $x$ minimization step for $\\hat{z} = z^k$ becomes:**\n",
    "\n",
    "$$\n",
    "\\text{argmin}_x \\ x^TPx + q^Tx + \\chi_{l \\leq z \\leq u}(\\hat{z}) + \\lambda^T(\\hat{z}-Ax) + \\frac{1}{2}\\rho \\|\\hat{z}-Ax\\|_2^2\n",
    "$$\n",
    "\n",
    "And then, since $\\hat{z}$ is fixed and feasible:\n",
    "$$\n",
    "\\text{argmin } x^TPx + q^Tx + \\lambda^T(\\hat{z}-Ax) + \\frac{1}{2}\\rho \\|\\hat{z}-Ax\\|_2^2\n",
    "$$\n",
    "\n",
    "This is a convex, differentiable, quadratic minimization problem\n",
    "\n",
    "* It can be tackled via gradient descent\n",
    "* ...Or by solving a linear system of equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The ADMM Steps for QP\n",
    "\n",
    "**The $z$ minimization step for $\\hat{x} = x^{k+1}$ becomes**\n",
    "\n",
    "$$\n",
    "\\text{argmin}_z \\ \\hat{x}^T P\\hat{x} + q^T \\hat{x} + \\chi_{l \\leq z \\leq u}(z) + \\lambda^T(z-A \\hat{x}) + \\frac{1}{2}\\rho \\|z-A \\hat{x}\\|_2^2\n",
    "$$\n",
    "\n",
    "Since $\\hat{x}$ is fixed, this can be reformulated as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin } & \\lambda^T z + \\frac{1}{2} \\rho\\|z-A\\hat{x}\\|_2^2 \\\\\n",
    "\\text{subject to: } & l \\leq z \\leq u\n",
    "\\end{align}$$\n",
    "\n",
    "...And finally separated in to $n$ problems (one per variable) in the form:\n",
    "$$\n",
    "\\text{argmin}_{z_j} \\left\\{ \\lambda_j z_j + \\frac{1}{2}\\rho(z_j-A_j\\hat{x})^2 \\mid l \\leq z_j \\leq u \\right\\} \n",
    "$$\n",
    "* These are all very easy to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Considerations\n",
    "\n",
    "**We used the ADMM to break QP into a sequence of simpler problems**\n",
    "\n",
    "The method can be used in other clever ways:\n",
    "\n",
    "* Optimization with non-differentiable reguralizers\n",
    "* Parallel training, by splitting examples into multiple problems\n",
    "* ...And using constraints to reach a consensus\n",
    "\n",
    "**The ADMM is best used for convex problems**\n",
    "\n",
    "* Classical results are for convex problems only\n",
    "* There are some (local) results non non-convex problems (e.g. [this one](https://ieeexplore.ieee.org/abstract/document/7239586))\n",
    "* ...But in practice it's less reliable\n",
    "\n",
    "**About the convergence pace**\n",
    "\n",
    "* It's very fast in the first iterations, but much slower later\n",
    "* You can high-quality solutions early, but reaching the optimum takes long\n",
    "* All in all, it's best to use the ADMM as an approximate method"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
